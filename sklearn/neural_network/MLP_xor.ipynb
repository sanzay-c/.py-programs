{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0bcb73-a590-4b29-8fc9-f1dccdcb15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a28cbec-c5c9-4070-8bb9-e514d2592023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y, here y is the target value\n",
    "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c013db76-1bb2-4cd4-8a43-ae6170ae85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeding = np.random.seed(42)\n",
    "input_dim = 2\n",
    "hidden_dim = 2\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3429287-deac-4bc9-9ac0-35871bd8506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight of input layer to hidden layer:\n",
      " [[0.43194502 0.29122914]\n",
      " [0.61185289 0.13949386]]\n",
      "--------------------------------\n",
      "Bias of hidden layer:\n",
      " [[0.29214465 0.36636184]]\n",
      "--------------------------------\n",
      "weight of hidden layer to output layer:\n",
      " [[0.45606998]\n",
      " [0.78517596]]\n",
      "--------------------------------\n",
      "Bias for output layer: \n",
      " [[0.19967378]]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the weights between input and hidden layer\n",
    "weights_input_hidden = np.random.rand(input_dim, hidden_dim)\n",
    "print(f\"weight of input layer to hidden layer:\\n {weights_input_hidden}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# there are two bias hidden for the hidden layer, there are two neurons in the hidden layer each has its own bias.\n",
    "bias_hidden = np.random.rand(1, hidden_dim)\n",
    "print(f\"Bias of hidden layer:\\n {bias_hidden}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# the weights between hidden to output layer\n",
    "weights_hidden_output = np.random.rand(hidden_dim, output_dim)\n",
    "print(f\"weight of hidden layer to output layer:\\n {weights_hidden_output}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# here is one bias for the output layer \n",
    "bias_output = np.random.rand(1, output_dim)\n",
    "print(f\"Bias for output layer: \\n {bias_output}\")\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15091a2c-05b3-4c93-9c5c-cd41cd979224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function using sigmoid, \n",
    "# this activation function is sometime called as an threshold, but most commonly called as activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivation(z):\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40920e64-62db-44a3-be83-edcd38adcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the MLP\n",
    "epochs = 10000 # number of iterations\n",
    "learning_rate = 0.1  # too much learning rate is an issue becaouse it jumps the step and too low is also a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "874fb27e-c37c-40ed-9f57-c2e9fce8574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.00040311685366260514\n",
      "Epoch 1000, Loss: 0.00039092072581335945\n",
      "Epoch 2000, Loss: 0.00037942540295763516\n",
      "Epoch 3000, Loss: 0.0003685727602412615\n",
      "Epoch 4000, Loss: 0.0003583108900281633\n",
      "Epoch 5000, Loss: 0.0003485932955817068\n",
      "Epoch 6000, Loss: 0.00033937820669781624\n",
      "Epoch 7000, Loss: 0.00033062799634776097\n",
      "Epoch 8000, Loss: 0.0003223086813929868\n",
      "Epoch 9000, Loss: 0.0003143894935987066\n",
      " delta predicted output: \n",
      "[[-0.00036521]\n",
      " [ 0.00027351]\n",
      " [ 0.00027339]\n",
      " [-0.00029366]]\n",
      "---------------------------------------------------\n",
      " weihght of hidden layer to output layer: \n",
      "[[  9.57580507 -10.28162036]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # ------FORWARD PROPOGATION------\n",
    "    # --- data flow from input layer to hidden layer, here the calculation of the hidden layer\n",
    "    #     using the formula: data_of_input_layer * weight_connecting_to_input_hidden + bias_of_hidden_layer---\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    \n",
    "    # checking the hidden layer output with the activation function\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input) \n",
    "\n",
    "    # --- data flow from hidden layer to output layer, here is the calucation of the output layer\n",
    "    #     using the formula: hidden_layer_output * weight_connecting_to_hidden_to_output_layer + bias_of_output ---\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    \n",
    "    # checking the hidden layer output with the activation function\n",
    "    predicted_output = sigmoid(output_layer_input) # final output of the forward propogation, but predicted one.\n",
    "    \n",
    "    # ------computer loss (mean squared error)------\n",
    "    loss = np.mean((y - predicted_output) ** 2)\n",
    "\n",
    "    # ------this is error of the output i.e target_value - predicted_output_\n",
    "    error_output = y - predicted_output\n",
    "\n",
    "    # ------BACK PROPOGATION------\n",
    "    # --- delta or error_signal measures how wrong the neuron is, how far it is from the target\n",
    "    #     this is dealta_predicted_output, calculated with:\n",
    "    #     error_of_output * sigmoid_derivation_of_predicted_output ---\n",
    "    # --- Calculate delta for the output layer (error signal) ---\n",
    "    d_predicted_output = error_output * sigmoid_derivation(predicted_output)\n",
    "    \n",
    "    # --- Calculate delta for the hidden layer---\n",
    "    # --- np.dot(d_predicted_output, weights_hidden_output.T) i.e.\n",
    "    #     delta_predicted_output * weights_hidden_to_output.T, \n",
    "    #     here T => transpose of matrix beacuse column cannot multiply with column so \".T \" (transpose) is applied to make it in a row,\n",
    "    #     to make the calculation row * column. ---\n",
    "    error_hidden_layer = np.dot(d_predicted_output, weights_hidden_output.T)\n",
    "    # --- Calculate delta for the hidden layer (error signal) ---\n",
    "    # **sigmoid_derivation(hidden_layer_output) => actaul hidden layer output from the forward propogation\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivation(hidden_layer_output)\n",
    "\n",
    "    # ------UPDATE WEIGHT AND BIASES------\n",
    "    # Update weights and biases for the input to hidden layer\n",
    "    # --- adjusting weight transpose of X(input) * delta_hidden_layer * learning_rate\n",
    "    weights_input_hidden += np.dot(X.T, d_hidden_layer) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Update weights and biases for the hidden to output layer\n",
    "    # --- adjusting weight transpose of hidden_layer_output(forward_propogation) * delta_predicted_output * learning_rate\n",
    "    weights_hidden_output += np.dot(hidden_layer_output.T, d_predicted_output) * learning_rate\n",
    "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Print loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(f\" delta predicted output: \\n{d_predicted_output}\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\" weihght of hidden layer to output layer: \\n{weights_hidden_output.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f59b8747-fd88-492f-8f01-9da394afeed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Predictions:\n",
      "Input: [0 0], Predicted Output: 0.053, Actual: 0\n",
      "Input: [0 1], Predicted Output: 0.952, Actual: 1\n",
      "Input: [1 0], Predicted Output: 0.952, Actual: 1\n",
      "Input: [1 1], Predicted Output: 0.052, Actual: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the trained MLP\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for i in range(len(X)):\n",
    "    hidden_layer_input = np.dot(X[i], weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {predicted_output[0][0]:.3f}, Actual: {y[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609ac33-67cc-47e0-8102-824b9fca598b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
